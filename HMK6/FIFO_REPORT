						Nate Ashby
						CS3100
						FIFO Report

In theory, the FIFO scheduling algorithm is feasible but certainly lacks in the area of response time. Since we have to wait for everything else on the queue to clear before the task responds, it takes quite a while for that event to occur. However, this means that the throughput should be reasonable, but then the program will be doing all different sizes of programs, instead of the smallest ones like the standard LIFO algorithm that most OS's use. The FIFO shines in it utilization, because it's system resources will be used, unless it's waiting on a device. So overall I'd say this algorithm is an acceptable way of scheduling things by the latency standard. It's high utilization and average throughput will provide a higher than average latency. The loss occurs on the size of the response time, because all of the tasks that have been placed on the ready queue must be completed before the last one can even respond.
But in practice, I saw that the utilization was nowhere near what I expected, and the number one reason was because the system couldn't effeciently use all of it's different IO devices. Even with 10 CPU's and 10 IO devices, I only saw a 50% utilization. The latency didn't seem to change as much as I expected when messing with the different values. The throughput tended to increase as I increased the percentage of CPU to IO intensive bursts. And the response time was always horrid, just as I suspected in my theory above. It was always above 75% of the latency, which would be really bad for any applications that take user input. If it was Fortran just crunching numbers, it might be ok; but even then, it's pretty awful.
Since I'm writing this in VIM I'm not entirely sure how long it is. However I think this is a pretty good explanation, and fulfills the assignment requirements.
